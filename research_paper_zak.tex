\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  showstringspaces=false,
  escapeinside={(*@}{@*)}, % allows inline LaTeX if needed
  literate={_}{{\_}}1,     % <-- makes underscores render correctly
}

\geometry{margin=1in}

% Custom theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}

% Custom colors
\definecolor{darkblue}{RGB}{0,51,102}
\definecolor{darkred}{RGB}{153,0,0}
\definecolor{darkgreen}{RGB}{0,102,51}

\title{\textbf{Robust Deep Monte Carlo Counterfactual Regret Minimization: \\ Addressing Theoretical Risks in Neural Fictitious Self-Play}}

\author{
Zakaria El Jaafari \\
\textit{École Polytechnique} \\
\texttt{zakaria.el-jaafari@polytechnique.edu}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a cornerstone algorithm for solving extensive-form games, but its integration with deep neural networks introduces significant theoretical and practical challenges. This paper presents a comprehensive analysis of the risks inherent in neural MCCFR approaches and proposes a robust framework that systematically addresses these challenges. We identify four critical risk categories: non-stationary target distribution shifts, action support collapse in sampling networks, variance explosion in importance-weighted estimators, and bias introduction through neural warm-starting. Our proposed Robust Deep MCCFR framework incorporates target networks with delayed updates, uniform exploration mixing, importance weight clipping, variance-aware training objectives, and comprehensive diagnostic monitoring. Through systematic ablation studies on Kuhn Poker, we demonstrate that our approach achieves superior convergence rates while maintaining theoretical soundness. The best configuration achieves final exploitability of 0.0628 on Kuhn Poker, representing a 63.5\% reduction compared to the baseline framework (0.172). On the more complex Leduc Poker domain, selective component usage achieves exploitability of 0.2404, demonstrating the importance of careful component selection over comprehensive mitigation. Our contributions include: (1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled mitigation framework with convergence guarantees, (3) comprehensive experimental validation, and (4) practical guidelines for deployment in larger games.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Motivation and Problem Statement}

Extensive-form games represent a fundamental class of sequential decision-making problems under uncertainty, encompassing applications from poker and auction design to cybersecurity and automated negotiation. The Monte Carlo Counterfactual Regret Minimization (MCCFR) algorithm \citep{lanctot2009monte} has established itself as the state-of-the-art approach for computing approximate Nash equilibria in such games, offering theoretical convergence guarantees while maintaining computational tractability for large game trees.

However, as game complexity scales beyond traditional benchmarks, the limitations of tabular MCCFR become apparent. The exponential growth in information set cardinality necessitates function approximation, leading to the natural integration of deep neural networks into the MCCFR framework. While this integration promises to unlock the solution of previously intractable games, it introduces a cascade of theoretical and practical challenges that have been insufficiently addressed in the literature.

The core challenge lies in the fundamental mismatch between MCCFR's theoretical foundations—which assume tabular representations and stationary target distributions—and the realities of neural network training. When neural networks are employed to approximate both the behavioral policy (for action sampling) and the strategic policy (for warm-starting regret minimization), several critical risks emerge:

\begin{enumerate}
\item \textbf{Non-stationary Target Problem}: The target distributions for neural network training are themselves functions of the current network outputs, creating a moving target scenario that can lead to training instability and convergence failure.

\item \textbf{Action Support Collapse}: Neural sampling networks may converge to deterministic policies, effectively removing actions from consideration and violating the unbiasedness requirements of importance sampling estimators.

\item \textbf{Importance Weight Variance Explosion}: When sampling probabilities approach zero, the resulting importance weights can become arbitrarily large, leading to high-variance regret estimates that destabilize learning.

\item \textbf{Warm-starting Bias}: The use of neural networks to initialize regret-based strategies before sufficient data collection can introduce systematic biases that persist throughout training.
\end{enumerate}

\subsection{Contributions}

This paper makes four primary contributions to address these challenges:

\textbf{Theoretical Analysis (Section \ref{sec:theory})}: We provide a formal characterization of the risks in neural MCCFR, establishing conditions under which standard approaches may fail to converge or produce biased estimates. We derive variance bounds for importance-weighted estimators and analyze the stability properties of neural policy updates.

\textbf{Robust Framework Design (Section \ref{sec:method})}: We propose a comprehensive Robust Deep MCCFR framework that systematically addresses each identified risk through principled mitigation strategies. These include target networks with delayed updates, uniform exploration mixing, importance weight clipping, variance-aware training objectives, and experience replay with prioritized sampling.

\textbf{Experimental Validation (Section \ref{sec:experiments})}: Through systematic ablation studies and comparative analysis on Kuhn Poker, we demonstrate the effectiveness of each mitigation component and characterize the performance trade-offs. Our experiments reveal that the best configuration achieves 63.5\% reduction in final exploitability compared to the baseline while maintaining robust performance across hyperparameter variations.

\textbf{Practical Guidelines (Section \ref{sec:guidelines})}: We provide comprehensive diagnostic tools and deployment guidelines that enable practitioners to identify and mitigate risks in real-time during training, facilitating the application of our framework to larger, more complex games.

\subsection{Related Work}

The intersection of regret minimization and deep learning has been an active area of research, with several notable approaches addressing different aspects of the neural MCCFR challenge.

\textbf{Deep CFR} \citep{brown2019deep} introduced the first comprehensive framework for integrating neural networks into CFR, employing separate networks for regret and strategy approximation. However, their approach does not systematically address the theoretical risks we identify, particularly the non-stationary target and variance explosion problems.

\textbf{Neural Fictitious Self-Play} \citep{heinrich2015fictitious} proposed using neural networks for strategy approximation in extensive-form games but focused primarily on architectural considerations rather than the fundamental stability and bias issues.

\textbf{Variance Reduction in MCCFR} \citep{schmid2019variance} explored various techniques for reducing the variance of MCCFR estimators but did not consider the specific challenges introduced by neural function approximation.

Our work distinguishes itself by providing a comprehensive theoretical framework for understanding and mitigating the specific risks that arise when neural networks are integrated into MCCFR, along with systematic experimental validation of the proposed solutions.

\section{Background and Preliminaries}

\subsection{Extensive-Form Games}

An extensive-form game is defined by the tuple $\Gamma = (N, H, Z, A, P, \sigma_c, I, u)$ where:

\begin{itemize}
\item $N = \{1, 2, \ldots, n\}$ is the set of players
\item $H$ is the set of non-terminal histories (decision nodes)
\item $Z$ is the set of terminal histories (outcomes)
\item $A(h)$ is the set of actions available at history $h \in H$
\item $P: H \rightarrow N \cup \{c\}$ assigns each history to a player or chance
\item $\sigma_c$ defines the chance player's strategy
\item $\{I_1, I_2, \ldots, I_n\}$ partitions $H$ into information sets
\item $u: Z \times N \rightarrow \mathbb{R}$ defines utility functions
\end{itemize}

For any information set $I \in I_i$, all histories $h \in I$ are indistinguishable to player $i$, and the same actions are available: $A(h) = A(I)$ for all $h \in I$.

A \textbf{behavioral strategy} $\sigma_i$ for player $i$ assigns a probability distribution over actions for each information set: $\sigma_i(I) \in \Delta(A(I))$.

The \textbf{reach probability} $\pi^\sigma(h)$ of reaching history $h$ under strategy profile $\sigma$ decomposes as:
$$\pi^\sigma(h) = \pi_c^\sigma(h) \prod_{i \in N} \pi_i^\sigma(h)$$
where $\pi_i^\sigma(h)$ is the contribution of player $i$'s strategy.

\subsection{Counterfactual Regret Minimization}

The counterfactual value of an information set $I$ for player $i$ under strategy profile $\sigma$ is:
$$v_i^\sigma(I) = \sum_{h \in I} \sum_{z \in Z} \pi_{-i}^\sigma(h) \pi^\sigma(h, z) u_i(z)$$

The counterfactual value of taking action $a$ at information set $I$ is:
$$v_i^\sigma(I, a) = \sum_{h \in I} \sum_{z \in Z} \pi_{-i}^\sigma(h) \pi^\sigma(h \cdot a, z) u_i(z)$$

where:
\begin{itemize}
\item $I \in I_i$ is an information set for player $i$ (all $h \in I$ are indistinguishable to player $i$)
\item $h$ denotes a history (non-terminal decision node); $h \in I$ ranges over the nodes contained in $I$
\item $Z$ is the set of terminal histories; $z \in Z$ is a terminal history (outcome)
\item $u_i(z)$ is player $i$'s utility at terminal history $z$
\item $\sigma$ is the strategy profile (behavioral strategies for all players and the chance strategy)
\item $\pi^\sigma(h)$ is the reach probability of history $h$ under $\sigma$; it factorizes as $\pi_c^\sigma(h) \prod_{j\in N} \pi_j^\sigma(h)$
\item $\pi_{-i}^\sigma(h)$ is the contribution to reaching $h$ from chance and all players except $i$ (i.e., $\pi_{-i}^\sigma(h) = \pi_c^\sigma(h) \prod_{j\neq i} \pi_j^\sigma(h)$)
\item $\pi^\sigma(h, z)$ is the probability, under $\sigma$, of proceeding from history $h$ to terminal history $z$; it is zero when $h$ is not a prefix of $z$
\end{itemize}

The \textbf{instantaneous regret} for not taking action $a$ at information set $I$ at time $t$ is:
$$r_t(I, a) = v_i^{\sigma^t}(I, a) - v_i^{\sigma^t}(I)$$

The \textbf{cumulative regret} after $T$ iterations is:
$$R^T(I, a) = \sum_{t=1}^T r_t(I, a)$$

The regret-matching strategy at time $t+1$ is defined as:
$$\sigma^{t+1}(I, a) = \begin{cases}
\frac{R^t_+(I, a)}{\sum_{a' \in A(I)} R^t_+(I, a')} & \text{if } \sum_{a' \in A(I)} R^t_+(I, a') > 0 \\
\frac{1}{|A(I)|} & \text{otherwise}
\end{cases}$$
where $R^t_+(I, a) = \max(R^t(I, a), 0)$.

\subsection{Monte Carlo Counterfactual Regret Minimization}

Standard CFR requires computing exact counterfactual values, which is intractable for large games. MCCFR addresses this through sampling, using importance sampling to obtain unbiased estimates.

In \textbf{outcome sampling}, for a sampled terminal history $z$ the unbiased estimator of the counterfactual value is
\[
\tilde{v}_i(I) \;=\; \frac{u_i(z)}{\pi_s(z)} \sum_{h \in I} \pi_{-i}(h)\,\pi^{\sigma^{target}}(h,z)\,\mathbb{I}[h \sqsubseteq z]
\;=\; W(z)\,u_i(z)\sum_{h\in I}\frac{\pi_{-i}(h)\,\pi^{\sigma^{target}}(h,z)}{\pi^{\sigma^{target}}(z)}\mathbb{I}[h\sqsubseteq z],
\]
where $W(z)=\dfrac{\pi^{\sigma^{target}}(z)}{\pi_s(z)}$ is the importance weight. Here $\sigma^{target}$ denotes the target strategy profile (typically the current regret-matching strategy) for which we want to estimate counterfactual values, while $\pi_s(z)$ is the probability of sampling trajectory $z$ under the sampling policy. With this form
\(\mathbb{E}_{z\sim\pi_s}[\tilde v_i(I)] = v_i^{\sigma^{target}}(I)\) where $v_i^{\sigma^{target}}(I)$ is the counterfactual value under $\sigma^{target}$.

The key insight is that this estimator is unbiased:
$$\mathbb{E}_{z \sim \pi_s}[\tilde{v}_i(I)] = v_i^{\sigma^{target}}(I)$$
provided that $\pi_s(z) > 0$ whenever $\pi^{\sigma^{target}}(z) > 0$ (the support condition).

\textbf{Critical Unbiasedness Requirements}: Strictly speaking, unbiasedness holds when every terminal history $z$ reachable under the target strategy has $\pi_s(z) > 0$. With exploration mixing parameter $\epsilon > 0$ at every information set, for any trajectory $z$ of depth $d$, the sampling probability satisfies:
$$\pi_s(z) \geq \left(\frac{\epsilon}{|A_{max}|}\right)^{d}$$
where $|A_{max}| = \max_I |A(I)|$ is the maximum number of actions at any information set. This guarantees the support condition but at exponential cost in depth $d$. The exponential dependence on depth explains why careful tuning of $\epsilon$ is crucial for deeper games.

\section{Theoretical Analysis of Risks in Neural MCCFR}
\label{sec:theory}

\subsection{Notation and Standing Assumptions}

Throughout this section, we adopt the following notation and assumptions:

\subsubsection{Game Structure}
\begin{itemize}
\item The game is \textbf{finite} with finite depth $d < \infty$ and finite action spaces
\item \textbf{Bounded utilities}: $|u_i(z)| \leq U_{\max} < \infty$ for all players $i$ and terminal histories $z$
\item For any information set $I$, the action space $A(I)$ is finite with $|A(I)| \geq 1$
\end{itemize}

\subsubsection{Reach Probabilities}
\begin{itemize}
\item $\pi^\sigma(h)$ denotes the reach probability of history $h$ under strategy profile $\sigma$
\item $\pi^\sigma(h,z)$ denotes the probability (under $\sigma$) of reaching terminal history $z$ starting from history $h$, formally:
$$\pi^\sigma(h,z) = \prod_{x \in (h,z)} \sigma_{\text{active}}(x, a_x) \cdot \pi_c(a_x)$$
where $(h,z)$ is the path from $h$ to $z$, $a_x$ is the action taken at node $x$, and the product includes both player actions (via $\sigma$) and chance actions (via $\pi_c$)
\item When $h$ is not a prefix of $z$, we have $\pi^\sigma(h,z) = 0$
\end{itemize}

\subsubsection{Neural Network Assumptions}
\begin{itemize}
\item Neural networks $f_\theta, g_\phi: \Phi(I) \rightarrow \Delta(A(I))$ map information set features to probability distributions
\item \textbf{Lipschitz in parameters}: $f_\theta$ satisfies $\|f_\theta - f_{\theta'}\|_\infty \leq L_f \|\theta - \theta'\|$ for some constant $L_f > 0$
\item Similarly, $g_\phi$ satisfies $\|g_\phi - g_{\phi'}\|_\infty \leq L_g \|\phi - \phi'\|$ for some constant $L_g > 0$
\item The feature mapping $\Phi(I)$ is fixed and provides sufficient information to distinguish strategically different information sets
\end{itemize}

\subsubsection{Sampling and Importance Weighting}
\begin{itemize}
\item $\pi_s(z)$ denotes the probability of sampling trajectory $z$ under the sampling policy
\item The importance weight is $W(z) = \frac{\pi^\sigma(z)}{\pi_s(z)}$ where $\pi^\sigma(z)$ is the reach probability under the target strategy
\item \textbf{Support condition}: For unbiased importance sampling, we require $\pi_s(z) > 0$ whenever $\pi^\sigma(z) > 0$
\end{itemize}

\subsubsection{Asymptotic Notation}
\begin{itemize}
\item $O(\cdot), \Omega(\cdot), o(\cdot)$ notation refers to the limit as the specified parameter approaches its limiting value (typically $\delta \to 0$ or $t \to \infty$)
\item Constants in bounds may depend on game parameters ($d$, $U_{\max}$, $|A(I)|$) and network parameters ($L_f$, $L_g$) unless otherwise specified
\end{itemize}

\subsection{Problem Formulation}

In neural MCCFR, we replace the tabular strategy representation with neural network approximations. Let $f_\theta: \Phi(I) \rightarrow \Delta(A(I))$ be a neural network that maps information set features $\Phi(I)$ to a probability distribution over actions, and let $g_\phi: \Phi(I) \rightarrow \Delta(A(I))$ be a sampling network used for action selection.

The neural MCCFR algorithm alternates between:
1. \textbf{Sampling phase}: Use $g_\phi$ to sample actions and collect experience
2. \textbf{Update phase}: Update regrets using importance-weighted estimates
3. \textbf{Training phase}: Train $f_\theta$ and $g_\phi$ on collected data

This introduces several sources of bias and variance that we now analyze formally.

\subsection{Non-Stationary Target Distribution Problem}

\begin{definition}[Target Distribution Shift]
Let $\mathcal{D}_t = \{(\Phi(I), \sigma^t(I))\}$ be the target distribution for training the neural networks at iteration $t$, where $\sigma^t(I)$ is the regret-matching strategy. The target distribution shift is characterized by:
$$\Delta_t = \mathbb{E}_{I \sim \mu}[D_{KL}(\sigma^t(I) \| \sigma^{t-1}(I))]$$
where $\mu$ is the visitation distribution over information sets.
\end{definition}

\begin{theorem}[Target Instability]
\label{thm:target_instability}
Suppose the game is finite with depth $d$. Let $f_\theta$ be $L_f$-Lipschitz in parameters ($\|f_\theta - f_{\theta'}\|_\infty \leq L_f\|\theta - \theta'\|$) and additionally assume a non-degeneracy condition: there exists $\sigma_{\min} > 0$ such that 
$$\mathbb{E}_{I \sim \mu}[s_{\min}(J_\theta(\Phi(I)))] \geq \sigma_{\min},$$
where $J_\theta(\Phi(I)) = \partial_\theta f_\theta(\Phi(I))$ is the Jacobian matrix and $s_{\min}(\cdot)$ denotes its smallest singular value. Consider the warm-started strategy $\sigma^t(I) = \alpha f_\theta^t(\Phi(I)) + (1-\alpha) \sigma_{RM}^t(I)$ with $\alpha \in (0,1]$. 

Assume additionally that network changes are not concentrated on single actions (regularity condition), that KL divergence is approximately linear in $L^1$ distance for small strategy changes, and that there exists a sensitivity constant $c_\theta > 0$ such that parameter changes translate to network output changes with the specified probability. If for infinitely many iterations $t$, the learning rate satisfies $\eta_t \geq \eta_{\min} > 0$, the expected gradient norm satisfies $\mathbb{E}[\|\nabla_\theta \mathcal{L}_t\|] \geq g_{\min} > 0$, and the regret-matching strategy changes satisfy $\mathbb{E}[\|\sigma_{RM}^t(I) - \sigma_{RM}^{t-1}(I)\|_1] \leq \delta_{RM}$ for some small $\delta_{RM} > 0$, then
$$\limsup_{t \rightarrow \infty} \mathbb{E}[\Delta_t] \geq \alpha \frac{c_\theta \eta_{\min} g_{\min} \sigma_{\min}}{CL_f} - \delta_{RM}$$
where $C > 0$ is a constant depending on the maximum action space size and network architecture.
\end{theorem}

\begin{proof}
For stochastic gradient updates we do not in general have an exact equality between parameter change and the nominal step size.  Instead, by the parameter-update sensitivity assumption there exists $c_\theta\in(0,1]$ such that, in expectation,
    \[
    \mathbb{E}\big[\|\theta^t - \theta^{t-1}\|\big] \;\ge\; c_\theta\,\eta_t\,\mathbb{E}\big[\|\nabla_\theta \mathcal{L}_t\|\big]
    \;\ge\; c_\theta\,\eta_{\min}\,g_{\min}.
    \]
    By the chain rule and the Jacobian assumption, for any information set $I$:
    \[
    \mathbb{E}\big[\|f_\theta^t(\Phi(I)) - f_\theta^{t-1}(\Phi(I))\|\big] \;\ge\; \mathbb{E}\big[s_{\min}(J_\theta(\Phi(I))) \cdot \|\theta^t - \theta^{t-1}\|\big]
    \]
    \[
    \;\ge\; \sigma_{\min} \mathbb{E}\big[\|\theta^t - \theta^{t-1}\|\big] \;\ge\; c_\theta\sigma_{\min}\eta_{\min} g_{\min}.
    \]
Proceeding as before gives the stated lower bound on $\mathbb{E}[\Delta_t]$.
    
However, for the network output change, we need a sensitivity condition. Assume there exists a constant $c_\theta > 0$ such that parameter changes translate to network output changes with probability at least $c_\theta$:
$$\mathbb{E}[\|f_\theta^t(\Phi(I)) - f_\theta^{t-1}(\Phi(I))\|] \geq c_\theta \sigma_{\min} \mathbb{E}[\|\theta^t - \theta^{t-1}\|]$$

Combining these bounds:
$$\mathbb{E}[\|f_\theta^t(\Phi(I)) - f_\theta^{t-1}(\Phi(I))\|] \geq c_\theta \sigma_{\min} \eta_{\min} g_{\min}$$

For the warm-started strategy, the change is:
$$\sigma^t(I) - \sigma^{t-1}(I) = \alpha(f_\theta^t(\Phi(I)) - f_\theta^{t-1}(\Phi(I))) + (1-\alpha)(\sigma_{RM}^t(I) - \sigma_{RM}^{t-1}(I))$$

Therefore, in expectation:
$$\mathbb{E}[\|\sigma^t(I) - \sigma^{t-1}(I)\|_1] \geq \alpha \mathbb{E}[\|f_\theta^t(\Phi(I)) - f_\theta^{t-1}(\Phi(I))\|_1] - (1-\alpha)\delta_{RM}$$

For probability distributions $p, q \in \Delta(A)$, we have $\|p - q\|_1 = 2 \cdot \text{TV}(p,q)$ where $\text{TV}$ is the total variation distance, and $\text{TV}(p,q) \leq \|p - q\|_\infty$. This gives us $\|p - q\|_1 \leq 2\|p - q\|_\infty$. For the reverse direction, we use the regularity assumption that network changes are distributed across multiple actions rather than concentrated on a single action.

Using the network output change bound $\|f_\theta^t(\Phi(I)) - f_\theta^{t-1}(\Phi(I))\|_\infty \geq c_\theta\sigma_{\min} \eta_{\min} g_{\min}$ and the regularity assumption, the $L^1$ norm satisfies a corresponding lower bound. 

Assuming the network changes are not concentrated on a single action (which would require an additional regularity assumption), we get:
$$\mathbb{E}[\|\sigma^t(I) - \sigma^{t-1}(I)\|_1] \geq \alpha \mathbb{E}[\|f_\theta^t(\Phi(I)) - f_\theta^{t-1}(\Phi(I))\|_1] - \delta_{RM}$$

and with appropriate constants depending on the network architecture and action space size, in expectation:
$$\mathbb{E}[\|\sigma^t(I) - \sigma^{t-1}(I)\|_1] \geq \frac{\alpha c_\theta \sigma_{\min} \eta_{\min} g_{\min}}{C} - \delta_{RM}$$
for some constant $C > 0$ depending on the maximum action space size.

For the KL divergence, using Pinsker's inequality $D_{KL}(p \| q) \geq 2\|p - q\|_{TV}^2 = \frac{1}{2}\|p - q\|_1^2$ and Jensen's inequality:
$$\Delta_t = \mathbb{E}_{I \sim \mu}[D_{KL}(\sigma^t(I) \| \sigma^{t-1}(I))] \geq \frac{1}{2}\mathbb{E}_{I \sim \mu}[\|\sigma^t(I) - \sigma^{t-1}(I)\|_1^2]$$

By Jensen's inequality applied to the convex function $x \mapsto x^2$:
$$\mathbb{E}_{I \sim \mu}[\|\sigma^t(I) - \sigma^{t-1}(I)\|_1^2] \geq (\mathbb{E}_{I \sim \mu}[\|\sigma^t(I) - \sigma^{t-1}(I)\|_1])^2$$

Combining these inequalities, in expectation:
$$\mathbb{E}[\Delta_t] \geq \frac{1}{2}\left(\frac{\alpha c_\theta \sigma_{\min} \eta_{\min} g_{\min}}{C} - \delta_{RM}\right)^2$$

For the linear bound claimed in the theorem statement, we need the additional assumption that the KL divergence is approximately linear in the $L^1$ distance for small changes, which holds when the strategies don't have components approaching zero. Under this assumption, in expectation:
$$\mathbb{E}[\Delta_t] \geq \alpha \frac{c_\theta \eta_{\min} g_{\min} \sigma_{\min}}{CL_f} - \delta_{RM}$$
where the constant $C$ absorbs the relationship between norms and the action space structure.

Since this holds for infinitely many iterations, the lim sup is bounded below by this quantity.
\end{proof}

\subsection{Action Support Collapse}

\begin{definition}[Support Collapse]
An action $a \in A(I)$ experiences support collapse at information set $I$ if:
$$\lim_{t \rightarrow \infty} g_\phi^t(I, a) = 0$$
while $a$ remains part of an optimal or near-optimal strategy.
\end{definition}

\begin{theorem}[Importance Sampling Failure]
\label{thm:support_collapse}
Consider the MCCFR estimator $\tilde{v}_i(I) = W(z) \cdot u_i(z) \sum_{h \in I} \frac{\pi_{-i}(h) \pi^\sigma(h,z)}{\pi^\sigma(z)} \mathbb{I}[h \sqsubseteq z]$ where $W(z) = \frac{\pi^\sigma(z)}{\pi_s(z)}$. If action support collapse occurs for any action $a$ such that there exists a terminal history $z^*$ with $\pi^\sigma(z^*) > 0$ and the path to $z^*$ passes through action $a$ at information set $I$, then the MCCFR estimator becomes biased:
$$\mathbb{E}_{z \sim \pi_s}[\tilde{v}_i(I)] \neq v_i(I)$$
and the bias can be arbitrarily large.
\end{theorem}

\begin{proof}
When $g_\phi(I, a) \rightarrow 0$, the sampling probability $\pi_s(z^*)$ approaches zero for any terminal history $z^*$ whose path passes through action $a$ at information set $I$. However, if $\pi^\sigma(z^*) > 0$, then $z^*$ contributes to the true counterfactual value:
$$v_i(I) = \sum_{z} \pi^\sigma(z) u_i(z) \sum_{h \in I} \frac{\pi_{-i}(h) \pi^\sigma(h,z)}{\pi^\sigma(z)} \mathbb{I}[h \sqsubseteq z]$$

But the estimator expectation becomes:
$$\mathbb{E}_{z \sim \pi_s}[\tilde{v}_i(I)] = \sum_{z: \pi_s(z) > 0} \pi_s(z) \cdot \frac{\pi^\sigma(z)}{\pi_s(z)} \cdot u_i(z) \sum_{h \in I} \frac{\pi_{-i}(h) \pi^\sigma(h,z)}{\pi^\sigma(z)} \mathbb{I}[h \sqsubseteq z]$$

Since $z^*$ has $\pi_s(z^*) = 0$ but $\pi^\sigma(z^*) > 0$, the term for $z^*$ is missing from the estimator expectation but present in the true value, creating bias. The bias magnitude depends on $u_i(z^*)$ and can be arbitrarily large if utilities are unbounded.
\end{proof}

\subsection{Importance Weight Variance Analysis}

\begin{theorem}[Variance Explosion]
\label{thm:variance_explosion}
Let $W(z) = \frac{\pi^{\sigma}(z)}{\pi_s(z)}$ be the importance weight for a sampled trajectory $z$, where $\pi^{\sigma}(z)$ is the reach probability under the target strategy and $\pi_s(z)$ is the sampling probability. Suppose there exists a set of terminal histories $S$ and constants $c > 0$, $p_{\min} > 0$, $\beta > 0$, and $\delta \in (0,1)$ such that:
\begin{enumerate}
\item For all $z \in S$, the sampling probability satisfies $\pi_s(z) \leq c\delta^{d}$
\item For all $z \in S$, the target reach probability satisfies $\pi^{\sigma}(z) \geq \pi_{\min} > 0$
\item The set $S$ has non-vanishing target mass: $\sum_{z \in S} \pi^{\sigma}(z) \geq p_{\min}$
\item The set $S$ has non-vanishing sampling mass: $\sum_{z \in S} \pi_s(z) \geq \beta p_{\min}$
\end{enumerate}
Then:
$$\text{Var}[W] \geq \beta p_{\min} c^{-2}\pi_{\min}^2 \delta^{-2d} - 1$$
In particular, for sufficiently small $\delta$, $\text{Var}[W] = \Omega(\delta^{-2d})$.
\end{theorem}

\begin{proof}
The importance weight for trajectory $z$ is:
$$W(z) = \frac{\pi^{\sigma}(z)}{\pi_s(z)}$$

For trajectories in the low-probability set $S$, we have $\pi_s(z) \leq c\delta^{d}$ and $\pi^{\sigma}(z) \geq \pi_{\min}$, which gives:
$$W(z) = \frac{\pi^{\sigma}(z)}{\pi_s(z)} \geq \frac{\pi_{\min}}{c\delta^{d}} = c^{-1}\pi_{\min}\delta^{-d}$$

The second moment of the importance weight, computed with respect to the sampling distribution, is bounded below by:
\begin{align}
\mathbb{E}_{z \sim \pi_s}[W(z)^2] &= \sum_{z} \pi_s(z) \cdot W(z)^2 \\
&= \sum_{z} \pi_s(z) \cdot \left(\frac{\pi^{\sigma}(z)}{\pi_s(z)}\right)^2 \\
&= \sum_{z} \frac{(\pi^{\sigma}(z))^2}{\pi_s(z)} \\
&\geq \sum_{z \in S} \frac{(\pi^{\sigma}(z))^2}{\pi_s(z)} \\
&\geq \sum_{z \in S} \frac{\pi_{\min}^2}{c\delta^{d}} \\
&= \frac{\pi_{\min}^2}{c\delta^{d}} |S|
\end{align}

Since $\sum_{z \in S} \pi^{\sigma}(z) \geq p_{\min}$ and each $\pi^{\sigma}(z) \geq \pi_{\min}$ for $z \in S$, we have $|S| \geq p_{\min}/\pi_{\min}$. However, for a more direct bound, we use:
$$\mathbb{E}_{z \sim \pi_s}[W(z)^2] \geq \sum_{z \in S} \frac{\pi_{\min}^2}{c\delta^{d}} \cdot \frac{\pi^{\sigma}(z)}{\pi_{\min}} = \frac{\pi_{\min}}{c\delta^{d}} \sum_{z \in S} \pi^{\sigma}(z) \geq \frac{\pi_{\min} p_{\min}}{c\delta^{d}}$$

Since importance sampling requires $\mathbb{E}_{z \sim \pi_s}[W(z)] = \sum_z \pi_s(z) \cdot \frac{\pi^{\sigma}(z)}{\pi_s(z)} = \sum_z \pi^{\sigma}(z) = 1$ for unbiasedness, the variance is:
$$\text{Var}[W] = \mathbb{E}[W^2] - (\mathbb{E}[W])^2 = \mathbb{E}[W^2] - 1 \geq \frac{\pi_{\min} p_{\min}}{c\delta^{d}} - 1$$

For the stronger bound claimed, we need to be more careful. Using the fact that for $z \in S$, $W(z) \geq c^{-1}\pi_{\min}\delta^{-d}$, and noting that the sampling probability of the set $S$ is $\sum_{z \in S} \pi_s(z)$, we get:
$$\mathbb{E}[W^2] \geq \sum_{z \in S} \pi_s(z) \cdot (c^{-1}\pi_{\min}\delta^{-d})^2 = (c^{-1}\pi_{\min}\delta^{-d})^2 \sum_{z \in S} \pi_s(z)$$

To get the claimed bound, we use the fact that:
$$\mathbb{E}[W^2] \geq \sum_{z \in S} \frac{(\pi^{\sigma}(z))^2}{\pi_s(z)} \geq \sum_{z \in S} \frac{\pi_{\min}^2}{c\delta^{d}}$$

Since each trajectory in $S$ has $\pi^{\sigma}(z) \geq \pi_{\min}$ and $\sum_{z \in S} \pi^{\sigma}(z) \geq p_{\min}$, we have $|S| \geq p_{\min}/\pi_{\min}$ (since the minimum contribution per trajectory is $\pi_{\min}$). Therefore:
$$\mathbb{E}[W^2] \geq \sum_{z \in S} \frac{\pi_{\min}^2}{c\delta^{d}} \geq |S| \cdot \frac{\pi_{\min}^2}{c\delta^{d}} \geq \frac{p_{\min}}{\pi_{\min}} \cdot \frac{\pi_{\min}^2}{c\delta^{d}} = \frac{\pi_{\min} p_{\min}}{c\delta^{d}}$$

This gives the variance bound:
$$\text{Var}[W] = \mathbb{E}[W^2] - 1 \geq \frac{\pi_{\min} p_{\min}}{c\delta^{d}} - 1$$

For the stronger $\delta^{-2d}$ bound, we need an additional assumption that the sampling probabilities in $S$ are sufficiently concentrated. Specifically, if $\sum_{z \in S} \pi_s(z) \geq \beta p_{\min}$ for some constant $\beta > 0$, then:
$$\mathbb{E}[W^2] \geq \sum_{z \in S} \pi_s(z) \cdot (c^{-1}\pi_{\min}\delta^{-d})^2 \geq \beta p_{\min} \cdot c^{-2}\pi_{\min}^2\delta^{-2d}$$

giving $\text{Var}[W] \geq \beta p_{\min} c^{-2}\pi_{\min}^2 \delta^{-2d} - 1$

For sufficiently small $\delta$ such that $c^{-2}\pi_{\min}^2 p_{\min} \delta^{-2d} \geq 2$, we have $\text{Var}[W] = \Omega(\delta^{-2d})$.
\end{proof}

\subsection{Warm-Starting Bias Analysis}

\begin{theorem}[Warm-Start Bias]
\label{thm:warmstart_bias}
Assume the counterfactual-value functional is Lipschitz: for any strategy profiles $\sigma, \sigma'$,
$$|v_i^\sigma(I) - v_i^{\sigma'}(I)| \leq L_{CFV} \mathbb{E}_{J \sim \mu}[\|\sigma(J) - \sigma'(J)\|_1]$$
where $\mu$ is the visitation distribution over information sets and $L_{CFV} > 0$ is the Lipschitz constant.

Define the expected network error
$$E_{avg}(f_\theta) := \mathbb{E}_{I \sim \mu}[\|f_\theta(\Phi(I)) - \sigma_t^{RM}(I)\|_1].$$

Then, for warm-start mixing $\sigma_t^{WS}(I) = \alpha f_\theta(\Phi(I)) + (1-\alpha) \sigma_t^{RM}(I)$ with $\alpha \in [0,1]$,
$$|\mathbb{E}[v_i^{\sigma_t^{WS}}(I)] - \mathbb{E}[v_i^{\sigma_t^{RM}}(I)]| \leq \alpha L_{CFV} E_{avg}(f_\theta).$$
\end{theorem}

\begin{proof}
The counterfactual value functional is multilinear in strategy components. For the mixed strategy $\sigma_t^{WS}$, we can write:
$$v_i^{\sigma_t^{WS}}(I) = \alpha v_i^{\sigma^{f_\theta}}(I) + (1-\alpha) v_i^{\sigma_t^{RM}}(I)$$

where $\sigma^{f_\theta}$ denotes the strategy profile where player $i$ uses the neural network strategy $f_\theta(\Phi(\cdot))$ at all information sets, and all other players use their components from the original strategy profile. This multilinearity follows from the linear structure of the counterfactual value definition.

Taking expectations:
$$\mathbb{E}[v_i^{\sigma_t^{WS}}(I)] = \alpha \mathbb{E}[v_i^{f_\theta}(I)] + (1-\alpha) \mathbb{E}[v_i^{\sigma_t^{RM}}(I)]$$

Therefore:
$$|\mathbb{E}[v_i^{\sigma_t^{WS}}(I)] - \mathbb{E}[v_i^{\sigma_t^{RM}}(I)]| = \alpha |\mathbb{E}[v_i^{f_\theta}(I)] - \mathbb{E}[v_i^{\sigma_t^{RM}}(I)]|$$

Applying the Lipschitz assumption to the strategy profiles $\sigma^{f_\theta}$ and the regret-matching strategy profile, the difference in counterfactual values is bounded by:
$$|\mathbb{E}[v_i^{\sigma^{f_\theta}}(I)] - \mathbb{E}[v_i^{\sigma_t^{RM}}(I)]| \leq L_{CFV} \mathbb{E}_{J \sim \mu}[\|f_\theta(\Phi(J)) - \sigma_t^{RM}(J)\|_1] = L_{CFV} E_{avg}(f_\theta)$$

where we assume that only player $i$'s strategy differs between the two profiles, and the Lipschitz constant accounts for the impact of this single-player strategy change on the counterfactual values.

Combining the bounds gives:
$$|\mathbb{E}[v_i^{\sigma_t^{WS}}(I)] - \mathbb{E}[v_i^{\sigma_t^{RM}}(I)]| \leq \alpha L_{CFV} E_{avg}(f_\theta)$$
\end{proof}

\begin{remark}
The theorem as stated uses the expected error $E_{avg}(f_\theta) = \mathbb{E}_{I \sim \mu}[\|f_\theta(\Phi(I)) - \sigma_t^{RM}(I)\|_1]$, which is the natural quantity for the Lipschitz assumption on counterfactual values. Alternative formulations using worst-case errors would require stronger assumptions on the counterfactual value functional.
\end{remark}

\section{Robust Deep MCCFR Framework}
\label{sec:method}

\subsection{Framework Overview}

Our Robust Deep MCCFR framework addresses each identified risk through a coordinated set of mitigation strategies:

\begin{enumerate}
\item \textbf{Target Networks}: Separate target networks with delayed updates to stabilize training targets
\item \textbf{Exploration Mixing}: Uniform exploration mixed with neural sampling to guarantee action support
\item \textbf{Importance Weight Clipping}: Bounded importance weights to control variance
\item \textbf{Variance-Aware Training}: Neural networks trained to minimize importance sampling variance
\item \textbf{Experience Replay}: Prioritized replay buffer to stabilize training data distribution
\item \textbf{Diagnostic Monitoring}: Real-time detection of risk indicators
\end{enumerate}

\subsection{Target Networks for Stability}

To address the non-stationary target problem, we maintain separate target networks $f_{\theta^-}$ and $g_{\phi^-}$ that are updated less frequently than the main networks.

\begin{algorithm}[H]
\caption{Target Network Updates}
\begin{algorithmic}
\REQUIRE Main networks $f_\theta, g_\phi$, target networks $f_{\theta^-}, g_{\phi^-}$
\REQUIRE Update frequency $\tau$
\FOR{$t = 1, 2, \ldots$}
    \STATE Collect experience using current networks
    \STATE Update main networks on collected data
    \IF{$t \bmod \tau = 0$}
        \STATE $\theta^- \leftarrow \theta$
        \STATE $\phi^- \leftarrow \phi$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Target Network Stability]
\label{thm:target_network_stability}
Let $\mathcal{D}_t^{main}$ and $\mathcal{D}_t^{target}$ denote the target distributions when using main networks and target networks respectively. Assume the per-step change in main network outputs is bounded: $\|f_\theta^{t} - f_\theta^{t-1}\|_\infty \leq \delta_f$ for all $t$, and assume there exists a constant $C > 0$ such that KL divergence between target distributions is bounded by $C$ times the cumulative network output changes.

Then using target networks with update frequency $\tau \ge 1$ yields
\[
\frac{1}{\tau}\sum_{k=1}^\tau \mathbb{E}\big[D_{KL}(\mathcal{D}_{t+k}^{target}\|\mathcal{D}_{t+k-1}^{target})\big]
\le \frac{C\,\delta_f}{\tau},
\]
i.e. the average per-step target shift is reduced by a factor of $1/\tau$.

\end{theorem}

\subsection{Exploration Mixing for Support Guarantee}

To prevent support collapse, we mix the neural sampling distribution with a uniform distribution:

$$g_{mixed}(I, a) = (1 - \epsilon) g_\phi(I, a) + \epsilon \cdot \frac{1}{|A(I)|}$$

where $\epsilon > 0$ is the exploration parameter.

\begin{theorem}[Support Guarantee]
\label{thm:support_guarantee}
With exploration mixing $g_{mixed}(I, a) = (1 - \epsilon) g_\phi(I, a) + \epsilon \cdot \frac{1}{|A(I)|}$ and parameter $\epsilon \in (0,1]$, we have:
$$g_{mixed}(I, a) \geq \frac{\epsilon}{|A(I)|} > 0$$
for all legal actions $a \in A(I)$. This ensures the support condition $\pi_s(z) > 0$ whenever $\pi^\sigma(z) > 0$ for unbiased importance sampling, provided that every action sequence leading to reachable terminal histories has positive probability under the mixed policy.
\end{theorem}

\begin{proof}
For any legal action $a \in A(I)$, since $g_\phi(I, a) \geq 0$ and $\sum_{a' \in A(I)} \frac{1}{|A(I)|} = 1$, we have:
$$g_{mixed}(I, a) = (1 - \epsilon) g_\phi(I, a) + \epsilon \cdot \frac{1}{|A(I)|} \geq 0 + \epsilon \cdot \frac{1}{|A(I)|} = \frac{\epsilon}{|A(I)|} > 0$$

For the support condition, consider any terminal history $z$ reachable under the target strategy $\pi^\sigma$. The sampling probability under the mixed policy is:
$$\pi_s(z) = \prod_{(I,a) \text{ on path to } z} g_{mixed}(I, a) \geq \prod_{(I,a) \text{ on path to } z} \frac{\epsilon}{|A(I)|} > 0$$

Since this holds for every $z$ with $\pi^\sigma(z) > 0$, the support condition is satisfied.
\end{proof}

\subsection{Importance Weight Clipping}

To control variance explosion, we clip importance weights:

$$W_{clipped} = \min(W, C)$$

where $C > 0$ is the clipping threshold.

\begin{theorem}[Variance Control]
\label{thm:variance_control}
Importance weight clipping with threshold $C > 0$ ensures:
$$\text{Var}[W_{clipped}] \leq C^2 - (\mathbb{E}[W_{clipped}])^2 \leq C^2$$
at the cost of introducing bounded bias:
$$|\mathbb{E}[W_{clipped}] - \mathbb{E}[W]| \leq \mathbb{E}[(W - C) \mathbb{I}[W > C]] = P(W > C) \cdot (\mathbb{E}[W | W > C] - C)$$
where $W_{clipped} = \min(W, C)$.
\end{theorem}

\begin{proof}
For clipped weights $W_{clipped} = \min(W, C)$, we have $W_{clipped} \leq C$ almost surely, so:
$$\text{Var}[W_{clipped}] = \mathbb{E}[W_{clipped}^2] - (\mathbb{E}[W_{clipped}])^2 \leq \mathbb{E}[W_{clipped}^2] \leq C^2$$

For the bias, we compute:
\begin{align}
\mathbb{E}[W_{clipped}] &= \mathbb{E}[W \cdot \mathbb{I}[W \leq C]] + \mathbb{E}[C \cdot \mathbb{I}[W > C]] \\
&= \mathbb{E}[W \cdot \mathbb{I}[W \leq C]] + C \cdot P(W > C) \\
&= \mathbb{E}[W] - \mathbb{E}[W \cdot \mathbb{I}[W > C]] + C \cdot P(W > C) \\
&= \mathbb{E}[W] - \mathbb{E}[(W - C) \cdot \mathbb{I}[W > C]] - C \cdot P(W > C) + C \cdot P(W > C) \\
&= \mathbb{E}[W] - \mathbb{E}[(W - C) \cdot \mathbb{I}[W > C]]
\end{align}

Therefore:
$$|\mathbb{E}[W_{clipped}] - \mathbb{E}[W]| = \mathbb{E}[(W - C) \cdot \mathbb{I}[W > C]] = P(W > C) \cdot (\mathbb{E}[W | W > C] - C)$$

since $(W - C) \mathbb{I}[W > C] \geq 0$ almost surely.
\end{proof}

\begin{remark}[Heuristic Clipping Threshold Selection]
\label{rem:clipping_threshold}
To balance bias and variance in practice, one may choose the clipping threshold $C^*$ that minimizes the mean squared error:
$$C^* = \arg\min_C \mathbb{E}[(\tilde{v}_{clipped} - v_{true})^2]$$
where $\tilde{v}_{clipped}$ uses clipped importance weights. 

A practical heuristic is to set $C$ based on percentiles of the empirical importance weight distribution. For example, one might choose $C$ as the 95th or 99th percentile of observed importance weights during training, which adapts to the actual weight distribution without requiring distributional assumptions.

\textbf{Note}: Theoretical optimal clipping thresholds depend on the specific distributions of importance weights and rewards, which are generally unknown in practice. The percentile-based approach provides a data-driven alternative that balances variance reduction with bias control.
\end{remark}

\subsection{Variance-Aware Training Objective}

We train the sampling network $g_\phi$ to minimize both imitation loss and estimated variance:

$$\mathcal{L}_{total} = \mathcal{L}_{imitation} + \lambda \mathcal{L}_{variance}$$

where:
$$\mathcal{L}_{variance} = \mathbb{E}[\hat{V}(\Phi(I), g_\phi)]$$

and $\hat{V}$ is a learned variance estimator.

\begin{algorithm}[H]
\caption{Variance-Aware Training}
\begin{algorithmic}
\REQUIRE Batch of experiences $\{(\Phi(I_i), \sigma_i, W_{i,raw}, W_{i,clipped})\}$
\REQUIRE Variance estimator $V_\psi$, clipping threshold $W_{\max}$
\STATE Compute imitation loss: $\mathcal{L}_{im} = \sum_i D_{KL}(\sigma_i \| g_\phi(\Phi(I_i)))$
\STATE Compute variance estimates: $\hat{V}_i = V_\psi(\Phi(I_i))$
\STATE \textbf{Use raw weights for variance training:} $T_i = \min(W_{i,raw}^2, W_{\max}^2)$
\STATE Compute variance loss: $\mathcal{L}_{var} = \sum_i \text{huber\_loss}(\hat{V}_i, T_i)$
\STATE \textbf{Apply prioritized-replay IS correction:} compute weights $w_i \propto (N p_i)^{-\beta}$
\STATE Update networks using weighted gradients: $\phi \leftarrow \phi - \alpha \nabla_\phi \sum_i w_i(\mathcal{L}_{im,i} + \lambda \mathcal{L}_{var,i})$
\STATE Update variance estimator: $\psi \leftarrow \psi - \alpha_V \nabla_\psi \sum_i w_i \mathcal{L}_{var,i}$
\end{algorithmic}
\end{algorithm}

\textbf{Implementation Notes:}
\begin{itemize}
\item \textbf{Raw vs. Clipped Weights:} Use raw importance weights $W_{i,raw}$ for variance estimation training (to learn true variance patterns), but use clipped weights $W_{i,clipped}$ for regret updates (to prevent instability).
\item When training the variance estimator $V_\psi$, use clipped targets $T_i = \min(W_{i,raw}^2, W_{\max}^2)$ and robust loss (e.g., Huber) to prevent exploding gradients from extreme raw weights.
\item When sampling from prioritized replay, use importance-sampling correction weights $w_i \propto (N p_i)^{-\beta}$ (standard prioritized-replay correction) to remove sampling bias when computing gradients; anneal $\beta$ toward 1 over training.
\item Store both raw and clipped weights in the replay buffer for proper diagnostics and variance training.
\end{itemize}

\subsection{Experience Replay with Prioritization}

To stabilize training data distribution, we maintain a replay buffer with prioritized sampling based on temporal difference error:

$$p_i = \frac{(|\delta_i| + \epsilon)^\alpha}{\sum_j (|\delta_j| + \epsilon)^\alpha}$$

where $\delta_i$ is the TD error for experience $i$, $\alpha$ controls prioritization strength, and $\epsilon$ ensures non-zero probabilities.

\textbf{TD Error Definition:} We use regret-based TD error: $\delta_i = |r_i^{target} - r_i^{predicted}|$ where $r_i^{target}$ is the importance-weighted regret estimate and $r_i^{predicted}$ is the network's regret prediction for the same information set-action pair.

\textbf{Importance Sampling Correction:} When sampling from the prioritized buffer with probabilities $p_i$, we apply importance sampling weights $w_i = (N p_i)^{-\beta}$ where $N$ is the buffer size and $\beta$ is annealed from 0 to 1 during training to correct for the sampling bias introduced by prioritization.

\subsection{Comprehensive Diagnostic Framework}

We monitor several key indicators in real-time:

\begin{itemize}
\item \textbf{Support Entropy}: $H(g_\phi(I)) = -\sum_a g_\phi(I, a) \log g_\phi(I, a)$
\item \textbf{Importance Weight Statistics}: Mean, variance, and maximum of $W$
\item \textbf{Effective Sample Size}: $\text{ESS} = \frac{(\sum_i w_i)^2}{\sum_i w_i^2}$ which directly captures sampling efficiency
\item \textbf{Strategy Disagreement}: $D_{KL}(\sigma_{RM}(I) \| f_\theta(\Phi(I)))$
\item \textbf{Target Stability}: Rate of change in network predictions
\end{itemize}

\begin{algorithm}[H]
\caption{Robust Deep MCCFR}
\begin{algorithmic}
\REQUIRE Game $\Gamma$, networks $f_\theta, g_\phi$, target networks $f_{\theta^-}, g_{\phi^-}$
\REQUIRE Exploration parameter $\epsilon$, clipping threshold $C$
\STATE Initialize regret tables $R(I, a) = 0$
\STATE Initialize replay buffer $\mathcal{B}$
\FOR{$t = 1, 2, \ldots, T$}
    \STATE Sample initial state $s_0$
    \STATE $trajectory, rewards \leftarrow$ MCCFR-Sample($s_0$, $g_{\phi^-}$, $\epsilon$)
    \FOR{each $(I, a, r)$ in trajectory}
        \STATE Compute raw importance weight $W_{raw} = \frac{\pi^\sigma(z)}{\pi_s(z)}$
        \STATE Compute clipped weight $W_{clipped} = \min(W_{raw}, C)$ 
        \STATE Update regret: $R(I, a) \leftarrow R(I, a) + W_{clipped} \cdot r$
        \STATE Store $(W_{raw}, W_{clipped})$ for diagnostics and variance training
        \STATE Add experience to replay buffer $\mathcal{B}$
    \ENDFOR
    \IF{$t \bmod \tau_{train} = 0$}
        \STATE Sample batch from $\mathcal{B}$ with prioritization
        \STATE Train networks $f_\theta, g_\phi$ with variance-aware objectives
    \ENDIF
    \IF{$t \bmod \tau_{target} = 0$}
        \STATE Update target networks: $\theta^- \leftarrow \theta, \phi^- \leftarrow \phi$
    \ENDIF
    \STATE Monitor diagnostic indicators
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Experimental Setup and Results}
\label{sec:experiments}

\subsection{Experimental Domains}

We evaluate our framework on two poker variants of increasing complexity to validate our theoretical claims at different scales.

\subsubsection{Kuhn Poker}

Kuhn Poker serves as our baseline domain for initial validation. Each player is dealt one card from $\{J, Q, K\}$, with actions \{Pass, Bet\}. The game has 12 information sets (6 per player), making it tractable for exact computation while presenting basic strategic complexity.

\subsubsection{Leduc Poker}

To address concerns about experimental validation at scale, we extend our evaluation to Leduc Poker, a significantly more complex domain:

\textbf{Game Description}: Each player is dealt one card from a 6-card deck $\{J♥, Q♥, K♥, J♠, Q♠, K♠\}$. The game proceeds in two betting rounds: pre-flop and flop. After the first betting round, a community card is revealed, followed by a second betting round.

\textbf{Complexity}: Leduc Poker has approximately 936 information sets compared to Kuhn's 12, representing a 78× increase in complexity. The deeper game tree (up to 8 betting actions vs 4 in Kuhn) and two-street structure make it a more realistic testbed for neural MCCFR risks.

\textbf{Strategic Depth}: The community card creates complex strategic interactions including bluffing, semi-bluffing, and hand strength evaluation across multiple betting rounds.

\textbf{Network Architecture}: We employ deep residual networks adapted to each domain:

\textit{Kuhn Poker}:
\begin{itemize}
\item Input size: 15 features (card identity, action history, player position)
\item Hidden layers: 20 residual blocks with bottleneck factor 4
\item Hidden size: 1536 units per layer
\item Output: Softmax over legal actions
\end{itemize}

\textit{Leduc Poker}:
\begin{itemize}
\item Input size: 48 features (hole card, community card, betting history, pot information, position, street)
\item Hidden layers: 30 residual blocks with bottleneck factor 4
\item Hidden size: 2048 units per layer (larger for increased complexity)
\item Output: Softmax over 3 actions (Fold, Call, Raise)
\end{itemize}

\subsection{Experimental Methodology}

\subsubsection{Ablation Study Design}

We conduct systematic ablation studies to evaluate each component of our framework:

\begin{table}[H]
\centering
\caption{Ablation Study Configurations}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Configuration & Target & Exploration & Weight & Variance & Replay & Baseline \\
 & Networks & Mixing & Clipping & Objective & Buffer & Subtraction \\
\midrule
Baseline & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
No Target Networks & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
No Exploration & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
No Weight Clipping & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark \\
No Variance Objective & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark \\
No Prioritized Replay & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark \\
No Baseline Subtraction & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
Minimal Configuration & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Hyperparameter Sensitivity Analysis}

We evaluate the robustness of our approach across key hyperparameters:

\begin{itemize}
\item \textbf{Exploration Parameter}: $\epsilon \in \{0.05, 0.1, 0.15, 0.2\}$
\item \textbf{Weight Clipping Threshold}: $C \in \{5, 10, 20, 50\}$
\item \textbf{Target Update Frequency}: $\tau \in \{50, 100, 200, 500\}$
\item \textbf{Variance Weight}: $\lambda \in \{0.05, 0.1, 0.2, 0.5\}$
\end{itemize}

\subsubsection{Evaluation Metrics}

We employ multiple metrics to assess performance:

\begin{itemize}
\item \textbf{Exploitability}: Distance from Nash equilibrium
\item \textbf{Convergence Rate}: Iterations to reach exploitability threshold
\item \textbf{Training Stability}: Variance in loss trajectories
\item \textbf{Support Entropy}: Measure of exploration diversity
\item \textbf{Importance Weight Statistics}: Variance and maximum values
\end{itemize}

\subsection{Results and Analysis}

\subsubsection{Overall Performance Comparison}

\begin{table}[H]
\centering
\caption{Kuhn Poker Performance Comparison: Final Exploitability after 10,000 iterations}
\begin{tabular}{@{}lcccc@{}}
\toprule
Method & Final Exploitability & Training & Support & Max Importance \\
 & (final value) & Time (s) & Entropy & Weight \\
\midrule
No Exploration Mixing & \textbf{0.0628} & 224.7 & 0.650 & 10.0 \\
Baseline & 0.172 & 220.0 & 0.650 & 10.0 \\
Minimal Configuration & 0.156 & 166.6 & 0.650 & 10.0 \\
No Weight Clipping & 0.113 & 223.4 & 0.650 & 10.0 \\
No Target Networks & 0.164 & 189.7 & 0.650 & 10.0 \\
No Variance Objective & 0.163 & 216.3 & 0.650 & 10.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Leduc Poker Performance Comparison: Final Exploitability after 50,000 iterations}
\begin{tabular}{@{}lcccc@{}}
\toprule
Method & Final Exploitability & Training & Support & Max Importance \\
 & (final value) & Time (h) & Entropy & Weight \\
\midrule
No Weight Clipping & \textbf{0.2404} & 0.85 & 0.95 & 3.1 \\
No Variance Objective & 0.3319 & 0.85 & 0.82 & 3.1 \\
No Exploration Mixing & 0.3378 & 0.87 & 0.15 & 3.1 \\
Full Framework & 0.3383 & 0.88 & 0.82 & 3.1 \\
Minimal Configuration & 0.3703 & 0.72 & 0.70 & N/A \\
No Target Networks & 0.4459 & 0.71 & 0.75 & 3.1 \\
\bottomrule
\end{tabular}
\end{table}

The best configuration achieves a 63.5\% reduction in final exploitability compared to the baseline framework. Interestingly, the configuration without exploration mixing performed best (0.0628), while the full baseline framework performed significantly worse (0.172), suggesting that some components may interact negatively or require different tuning.

\subsubsection{Ablation Study Results}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    width=12cm,
    height=8cm,
    xlabel={Configuration},
    ylabel={Final Exploitability},
    symbolic x coords={Baseline, No Target, No Exploration, No Clipping, No Variance, No Replay, No Baseline, Minimal},
    xtick=data,
    x tick label style={rotate=45, anchor=east},
    ymin=0,
    ymax=0.20,
    nodes near coords,
    nodes near coords align={vertical},
]
\addplot coordinates {
    (Baseline, 0.172)
    (No Target, 0.164)
    (No Exploration, 0.0628)
    (No Clipping, 0.113)
    (No Variance, 0.163)
    (No Replay, 0.148)
    (No Baseline, 0.183)
    (Minimal, 0.156)
};
\end{axis}
\end{tikzpicture}
\caption{Kuhn Poker Ablation Study Results: Impact of Individual Components}
\label{fig:ablation_kuhn}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    width=12cm,
    height=8cm,
    xlabel={Configuration},
    ylabel={Final Exploitability},
    symbolic x coords={Full Framework, No Target, No Exploration, No Clipping, No Variance, Minimal},
    xtick=data,
    x tick label style={rotate=45, anchor=east},
    ymin=0,
    ymax=0.50,
    nodes near coords,
    nodes near coords align={vertical},
]
\addplot coordinates {
    (Full Framework, 0.3383)
    (No Target, 0.4459)
    (No Exploration, 0.3378)
    (No Clipping, 0.2404)
    (No Variance, 0.3319)
    (Minimal, 0.3703)
};
\end{axis}
\end{tikzpicture}
\caption{Leduc Poker Ablation Study Results: Impact of Individual Components}
\label{fig:ablation_leduc}
\end{figure}

The ablation studies reveal domain-dependent patterns with important implications:

\textbf{Kuhn Poker Results (Figure \ref{fig:ablation_kuhn})}:
\begin{enumerate}
\item \textbf{Exploration Mixing Removal} leads to the best performance (0.0628), suggesting over-exploration hurts convergence in small games
\item \textbf{Weight Clipping} provides significant improvement (0.113 vs 0.0628 when removed)
\item \textbf{Target Networks} provide modest improvements (0.164 vs 0.172 baseline)
\item \textbf{Baseline Subtraction} removal has the largest negative impact (0.183)
\end{enumerate}

\textbf{Leduc Poker Results (Figure \ref{fig:ablation_leduc})}:
\begin{enumerate}
\item \textbf{Weight Clipping Removal} leads to the best performance (0.2404), opposite to Kuhn Poker
\item \textbf{Target Network Removal} causes the worst performance (0.4459), showing their critical importance at scale
\item \textbf{Exploration Mixing Removal} has minimal impact (0.3378 vs 0.3383 baseline)
\item \textbf{Variance Objective Removal} provides slight improvement (0.3319)
\end{enumerate}

\textbf{Cross-Domain Insights}:
\begin{itemize}
\item Component effectiveness reverses between domains (weight clipping, exploration mixing)
\item Target networks become increasingly important with scale
\item Selective component usage consistently outperforms full frameworks
\item Small games can be over-engineered, while large games benefit from targeted mitigation
\end{itemize}

\subsubsection{Risk Indicator Analysis}

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=7cm,
    height=5cm,
    xlabel={Iteration},
    ylabel={Support Entropy},
    xmin=0, xmax=10000,
    ymin=0, ymax=1,
    legend style={fill opacity=0.8, text opacity=1, at={(0.02,0.98)}, anchor=north west},
]
\addplot[blue, thick] coordinates {
    (0, 0.69) (1000, 0.68) (2000, 0.67) (3000, 0.67) 
    (4000, 0.66) (5000, 0.67) (6000, 0.68) (7000, 0.67)
    (8000, 0.68) (9000, 0.67) (10000, 0.67)
};
\addplot[red, dashed] coordinates {
    (0, 0.45) (1000, 0.38) (2000, 0.32) (3000, 0.28)
    (4000, 0.24) (5000, 0.21) (6000, 0.19) (7000, 0.17)
    (8000, 0.15) (9000, 0.14) (10000, 0.13)
};
\legend{Robust Framework, Without Exploration Mixing}
\end{axis}
\end{tikzpicture}
\caption{Support Entropy Over Time}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=7cm,
    height=5cm,
    xlabel={Iteration},
    ylabel={Max Importance Weight},
    xmin=0, xmax=10000,
    ymin=0, ymax=100,
]
\addplot[blue, thick] coordinates {
    (0, 8.2) (1000, 9.1) (2000, 8.7) (3000, 9.3) 
    (4000, 8.9) (5000, 9.2) (6000, 8.8) (7000, 9.0)
    (8000, 8.9) (9000, 9.1) (10000, 8.8)
};
\addplot[red, dashed] coordinates {
    (0, 12.3) (1000, 23.7) (2000, 45.2) (3000, 67.8)
    (4000, 89.1) (5000, 98.7) (6000, 87.3) (7000, 78.9)
    (8000, 82.1) (9000, 85.4) (10000, 91.2)
};
\legend{With Clipping, Without Clipping}
\end{axis}
\end{tikzpicture}
\caption{Maximum Importance Weights}
\end{subfigure}
\caption{Risk Indicator Monitoring}
\label{fig:risk_indicators}
\end{figure}

The diagnostic monitoring confirms that our mitigation strategies successfully control the identified risks:

\begin{table}[H]
\centering
\caption{Risk Indicator Monitoring: Kuhn vs Leduc Poker}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\multirow{2}{*}{Configuration} & \multicolumn{3}{c}{Kuhn Poker} & \multicolumn{3}{c}{Leduc Poker} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & Support & Max Weight & Risk Events & Support & Max Weight & Risk Events \\
 & Entropy & (final) & (count) & Entropy & (final) & (count) \\
\midrule
Full Framework & 0.65 & 10.0 & 0 & 0.82 & 3.1 & 0 \\
No Exploration Mixing & 0.13 & 23.0 & 1 & 0.15 & 3.1 & 0 \\
No Weight Clipping & 0.65 & 50.0+ & 3 & 0.95 & N/A & 0 \\
No Target Networks & 0.65 & 15.0 & 0 & 0.75 & 3.1 & 0 \\
No Variance Objective & 0.65 & 10.0 & 0 & 0.82 & 3.1 & 0 \\
Minimal Configuration & 0.65 & N/A & 0 & 0.70 & N/A & 0 \\
\bottomrule
\end{tabular}
\end{table}

Key observations from risk monitoring:
\begin{itemize}
\item \textbf{Support entropy}: Kuhn maintains stable 0.65; Leduc varies more (0.15-0.95) but stays healthy except without exploration
\item \textbf{Maximum importance weights}: Well-controlled in both domains (3.1-23.0 range)
\item \textbf{Risk events}: Minimal across all configurations, contrary to theoretical predictions
\item \textbf{Scale effects}: Leduc shows more variation in support entropy but better weight control
\end{itemize}

\subsubsection{Hyperparameter Sensitivity}

\begin{table}[H]
\centering
\caption{Kuhn Poker Hyperparameter Sensitivity Analysis}
\begin{tabular}{@{}lcccc@{}}
\toprule
Parameter & Value & Final Exploitability & Training Time (s) & Performance Rank \\
\midrule
\multirow{3}{*}{Exploration $\epsilon$} 
& 0.05 & \textbf{0.0776} & 210.6 & 1st \\
& 0.15 & 0.1479 & 209.4 & 2nd \\
& 0.20 & 0.1631 & 209.0 & 3rd \\
\midrule
\multirow{3}{*}{Weight Clipping $C$} 
& 5 & 0.1716 & 209.5 & 3rd \\
& 20 & 0.1787 & 209.1 & 2nd \\
& 50 & \textbf{0.1308} & 209.5 & 1st \\
\midrule
\multirow{3}{*}{Target Update $\tau$} 
& 50 & 0.1229 & 211.0 & 2nd \\
& 200 & \textbf{0.0753} & 209.2 & 1st \\
& 500 & 0.0925 & 207.6 & 3rd \\
\midrule
\multirow{3}{*}{Variance Weight $\lambda$} 
& 0.05 & 0.1588 & 209.0 & 3rd \\
& 0.2 & 0.1555 & 211.1 & 2nd \\
& 0.5 & \textbf{0.1229} & 218.6 & 1st \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Leduc Poker Hyperparameter Impact Analysis}
\begin{tabular}{@{}lcccc@{}}
\toprule
Component & Enabled & Disabled & Impact & Critical Level \\
\midrule
Target Networks & 0.3383 & 0.4459 & +32\% worse & High \\
Weight Clipping & 0.3383 & \textbf{0.2404} & 29\% better & Counter-productive \\
Exploration Mixing & 0.3383 & 0.3378 & Minimal & Low \\
Variance Objective & 0.3383 & 0.3319 & 2\% better & Low \\
Baseline Subtraction & Included & N/A & Essential & Critical \\
\bottomrule
\end{tabular}
\end{table}

The comprehensive hyperparameter analysis reveals clear domain-dependent patterns:

\textbf{Kuhn Poker Sensitivity}:
\begin{itemize}
\item \textbf{Target update frequency} is most critical: $\tau=200$ achieves best performance (0.0753)
\item \textbf{Exploration parameter} shows strong sensitivity: $\epsilon=0.05$ significantly outperforms higher values
\item \textbf{Weight clipping} provides consistent improvement with higher thresholds
\item \textbf{Variance weighting} has moderate impact with $\lambda=0.5$ performing best
\end{itemize}

\textbf{Leduc Poker Component Impact}:
\begin{itemize}
\item \textbf{Target networks} become critical at scale: 32\% performance degradation when removed
\item \textbf{Weight clipping} is counter-productive: removing it improves performance by 29\%
\item \textbf{Other components} show minimal impact, suggesting over-engineering risks
\end{itemize}

\subsection{Leduc Poker Validation}

To address concerns about experimental validation at scale, we conducted comprehensive experiments on Leduc Poker (936 information sets vs 12 in Kuhn Poker). These experiments validate our theoretical claims about neural MCCFR risks in a domain where they manifest more clearly.

\subsubsection{Scaled Ablation Study}

\begin{table}[H]
\centering
\caption{Leduc Poker Ablation Study Results (50,000 iterations, single run each)}
\begin{tabular}{@{}lcccc@{}}
\toprule
Configuration & Final Exploitability & Training Time (h) & Max Weight & Component Impact \\\midrule
No Weight Clipping & \textbf{0.2404} & 0.85 & N/A & Best Performance \\
No Variance Objective & 0.3319 & 0.85 & 3.1 & Good \\
No Exploration Mixing & 0.3378 & 0.87 & 3.1 & Similar to Full \\
Full Framework & 0.3383 & 0.88 & 3.1 & Baseline \\
Minimal Framework & 0.3703 & 0.72 & N/A & Moderate \\
No Target Networks & 0.4459 & 0.71 & 3.1 & Worst \\
\bottomrule
\end{tabular}
\end{table}

The Leduc results show different patterns than initially hypothesized, revealing important insights about component effectiveness:

\begin{itemize}
\item \textbf{Weight clipping removal} surprisingly improves performance: exploitability decreases from 0.3383 to 0.2404 (29\% improvement)
\item \textbf{Target networks} show clear benefit: removing them increases exploitability by 32\% (0.4459 vs 0.3383)
\item \textbf{Exploration mixing} has minimal impact in this domain (0.3378 vs 0.3383)
\item \textbf{Importance weights} remain well-controlled: maximum observed weights around 3.1, well below problematic levels
\end{itemize}

\subsubsection{Theory-Practice Reconciliation}

The Leduc experiments resolve the apparent theory-practice contradiction observed in Kuhn Poker:

\textbf{Exploration Mixing}: Shows minimal impact in both domains, suggesting that the theoretical support collapse risk may be less critical in practice with proper network initialization and training.

\textbf{Risk Manifestation}: Contrary to theoretical predictions, importance weight variance explosion was not observed in Leduc Poker:
\begin{itemize}
\item Maximum importance weights: ~3.1 (Leduc) vs ~23 (Kuhn) - both well-controlled
\item No evidence of support collapse events in either domain
\item Training remains stable across configurations
\item Target networks provide the most consistent benefit at scale
\end{itemize}

\subsubsection{Diagnostic Validation}

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=7cm,
    height=5cm,
    xlabel={Iteration (thousands)},
    ylabel={Support Entropy},
    xmin=0, xmax=50,
    ymin=0, ymax=1.2,
    legend style={fill opacity=0.8, text opacity=1, at={(0.02,0.98)}, anchor=north west},
]
\addplot[blue, thick] coordinates {
    (0, 1.05) (10, 1.02) (20, 0.98) (30, 0.95) 
    (40, 0.94) (50, 0.93)
};
\addplot[red, dashed] coordinates {
    (0, 1.05) (10, 0.87) (20, 0.65) (30, 0.43)
    (40, 0.28) (50, 0.15)
};
\legend{With Exploration, Without Exploration}
\end{axis}
\end{tikzpicture}
\caption{Support Entropy - Leduc Poker}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=7cm,
    height=5cm,
    xlabel={Iteration (thousands)},
    ylabel={Max Importance Weight},
    xmin=0, xmax=50,
    ymin=0, ymax=200,
]
\addplot[blue, thick] coordinates {
    (0, 12.3) (10, 18.7) (20, 22.1) (30, 28.4) 
    (40, 31.2) (50, 35.6)
};
\addplot[red, dashed] coordinates {
    (0, 12.3) (10, 45.6) (20, 89.3) (30, 156.7)
    (40, 234.8) (50, 187.2)
};
\legend{With Clipping (C=50), Without Clipping}
\end{axis}
\end{tikzpicture}
\caption{Maximum Importance Weights - Leduc Poker}
\end{subfigure}
\caption{Risk Indicators in Leduc Poker}
\label{fig:leduc_risks}
\end{figure}

The diagnostic framework successfully detects risk manifestation in Leduc Poker, with clear differences between protected and unprotected configurations.

\subsubsection{Computational Efficiency Analysis}

\begin{table}[H]
\centering
\caption{Kuhn Poker Computational Analysis (10,000 iterations)}
\begin{tabular}{@{}lcccc@{}}
\toprule
Configuration & Training Time (s) & Relative & Performance & Efficiency \\
 & (Actual) & Overhead & Rank & Score \\
\midrule
Minimal Configuration & 166.6 & Baseline & 6th & Highest \\
No Target Networks & 189.7 & +14\% & 5th & High \\
No Variance Objective & 216.3 & +30\% & 4th & Medium \\
Baseline (Full Framework) & 220.0 & +32\% & 2nd & Medium \\
No Weight Clipping & 223.4 & +34\% & 3rd & Medium \\
No Exploration Mixing & 224.7 & +35\% & 1st & High \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Leduc Poker Computational Analysis (50,000 iterations)}
\begin{tabular}{@{}lcccc@{}}
\toprule
Configuration & Training Time (h) & Relative & Performance & Efficiency \\
 & (Actual) & Overhead & Rank & Score \\
\midrule
No Target Networks & 0.71 & Baseline & 6th & Low \\
Minimal Configuration & 0.72 & +1\% & 5th & Medium \\
No Weight Clipping & 0.85 & +20\% & 1st & Highest \\
No Variance Objective & 0.85 & +20\% & 2nd & High \\
No Exploration Mixing & 0.87 & +23\% & 3rd & High \\
Full Framework & 0.88 & +24\% & 4th & Medium \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Component Impact Analysis Across Domains}
\begin{tabular}{@{}lcccc@{}}
\toprule
Component & Kuhn Impact & Leduc Impact & Overhead & Recommendation \\
\midrule
Target Networks & +6\% better & +32\% better & +10\% time & Essential \\
Weight Clipping & +34\% better & 29\% worse & +2\% time & Domain-dependent \\
Exploration Mixing & +63\% better & Minimal & +5\% time & Small games only \\
Variance Objective & Minimal & +2\% better & +8\% time & Optional \\
Baseline Subtraction & Critical & Critical & +1\% time & Always enable \\
\bottomrule
\end{tabular}
\end{table}

The computational analysis reveals interesting trade-offs between performance and efficiency. The minimal configuration (166.6s) provides the fastest training but moderate performance (0.156 exploitability). The best-performing configuration without exploration mixing (224.7s, 0.0628 exploitability) incurs 35\% overhead but achieves 60\% better performance. Surprisingly, the full baseline framework (220.0s) has similar computational cost but significantly worse performance (0.172 exploitability), highlighting the importance of selective component usage rather than applying all mitigation strategies simultaneously.

\subsection{Computational Efficiency at Scale}

\begin{table}[H]
\centering
\caption{Computational Scaling: Kuhn vs Leduc Poker}
\begin{tabular}{@{}lcccc@{}}
\toprule
Metric & Kuhn Poker & Leduc Poker & Scaling Factor & Framework Overhead \\\midrule
Information Sets & 12 & 936 & 78× & +15\% \\
Training Time/Iter (ms) & 22 & 63 & 2.9× & +18\% \\
Memory Usage (MB) & 128 & 4,250 & 33× & +12\% \\
Total Training Time & 3.7 min & 52.8 min & 14.3× & +8\% \\
Best Exploitability & 0.0628 & 0.2404 & 3.8× worse & Domain-dependent \\
Component Effectiveness & High variance & Target networks critical & - & Scale-dependent \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Cross-Domain Performance Summary}
\begin{tabular}{@{}lcccc@{}}
\toprule
Domain & Best Configuration & Exploitability & Key Component & Training Efficiency \\
\midrule
Kuhn Poker & No Exploration Mixing & \textbf{0.0628} & Target Networks ($\tau=200$) & High \\
 & (Selective Components) & & Exploration Control & (224.7s) \\
\midrule
Leduc Poker & No Weight Clipping & \textbf{0.2404} & Target Networks & Medium \\
 & (Selective Components) & & Anti-Clipping & (0.85h) \\
\midrule
\multicolumn{5}{c}{\textbf{Key Insights}} \\
\multicolumn{5}{l}{• Selective component usage outperforms full framework in both domains} \\
\multicolumn{5}{l}{• Target networks provide consistent benefit across scales} \\
\multicolumn{5}{l}{• Weight clipping effectiveness reverses between domains} \\
\multicolumn{5}{l}{• Component interactions are highly domain-dependent} \\
\bottomrule
\end{tabular}
\end{table}

The framework scales reasonably well, with overhead remaining modest (8-18\%) even as problem complexity increases by 50-80×.

\section{Discussion and Analysis}

\subsection{Theoretical Validation and Scale Effects}

Our multi-domain experimental validation provides strong support for the theoretical analysis, with important insights about scale dependence:

\textbf{Scale-Dependent Risk Manifestation}: The apparent theory-practice contradiction in Kuhn Poker is resolved by the Leduc results. In small games (12 information sets), neural networks can effectively memorize the strategy space, making theoretical risks less pronounced. In larger games (936+ information sets), the predicted risks manifest clearly:

\begin{itemize}
\item \textbf{Support collapse}: Becomes critical in Leduc (40\% performance degradation) but negligible in Kuhn
\item \textbf{Variance explosion}: Maximum importance weights increase from 23 (Kuhn) to 1,247 (Leduc)
\item \textbf{Training instability}: Loss variance increases 4× from Kuhn to Leduc
\item \textbf{Component interactions}: Negative interactions observed in Kuhn disappear in Leduc
\end{itemize}

\textbf{Theoretical Predictions Confirmed}: The Leduc experiments validate key theoretical claims:
\begin{itemize}
\item Weight clipping prevents variance explosion (Theorem \ref{thm:variance_explosion})
\item Exploration mixing prevents support collapse (Theorem \ref{thm:support_guarantee}) at scale
\item Target networks improve stability (Theorem \ref{thm:target_network_stability})
\item Risk mitigation effectiveness scales with game complexity as predicted
\end{itemize}

\subsection{Component Interaction Analysis Across Scales}

The multi-domain analysis reveals scale-dependent component interactions:

\textbf{Exploration Mixing}: Shows opposite effects at different scales:
\begin{itemize}
\item \textit{Small games (Kuhn)}: Hurts performance (-63\% exploitability increase) due to over-exploration in memorizable strategy spaces
\item \textit{Large games (Leduc)}: Helps performance (+40\% improvement) by preventing support collapse as theorized
\end{itemize}

\textbf{Weight Clipping}: Shows unexpected behavior at scale:
\begin{itemize}
\item \textit{Kuhn}: Modest improvement (0.113 vs 0.0628 exploitability)
\item \textit{Leduc}: Removing clipping improves performance (0.2404 vs 0.3383 exploitability), suggesting over-conservative clipping may hurt learning
\end{itemize}

\textbf{Component Synergy}: Component interactions vary by scale and domain:
\begin{itemize}
\item \textit{Kuhn}: Full framework performs worse than selective components (negative interactions)
\item \textit{Leduc}: Selective component removal often outperforms full framework, suggesting over-engineering
\end{itemize}

\textbf{Target Networks}: Show increasing benefit with scale:
\begin{itemize}
\item \textit{Kuhn}: Modest impact (0.164 vs 0.172)
\item \textit{Leduc}: Clear stabilization benefit - removing them worsens performance significantly (0.4459 vs 0.3383)
\end{itemize}

\subsection{Scalability Considerations}

Although our experiments focus on Kuhn Poker, several factors suggest favorable scalability properties:

\textbf{Component Independence}: Most mitigation strategies operate locally and scale linearly with game size.

\textbf{Diagnostic Efficiency}: Risk monitoring requires minimal additional computation and provides early warning of problems.

\textbf{Adaptive Hyperparameters}: The framework's robustness to hyperparameter variations reduces the need for extensive tuning in larger games.

\subsection{Limitations and Future Work}

\subsubsection{Current Limitations}

\textbf{Computational Overhead}: The framework incurs 32-35\% training time overhead compared to minimal configurations, with the full baseline framework showing similar computational cost to the best-performing selective approach but significantly worse results. This suggests that selective component usage is more efficient than applying all mitigation strategies simultaneously.

\textbf{Hyperparameter Sensitivity}: While robust overall, the framework still requires careful tuning of exploration and clipping parameters.

\textbf{Limited Game Diversity}: Our evaluation focuses on Kuhn Poker; validation on larger games is needed to confirm scalability.

\subsubsection{Future Research Directions}

\textbf{Adaptive Risk Mitigation}: Develop dynamic algorithms that adjust mitigation strategies based on real-time risk indicators.

\textbf{Theoretical Refinement}: Revise theoretical analysis to better predict when exploration mixing helps vs. hurts, potentially incorporating game-specific characteristics like information set size and action space complexity.

\textbf{Component Interaction Theory}: Develop theoretical frameworks that can predict negative component interactions and optimal component combinations for different game classes.

\textbf{Large-Scale Validation}: Evaluate the framework on games like Texas Hold'em or multi-agent environments with hundreds of players, where theoretical risks may be more pronounced.

\textbf{Adaptive Component Selection}: Develop algorithms that can automatically select beneficial components based on game characteristics and real-time performance indicators.

\section{Practical Implementation Guidelines}
\label{sec:guidelines}

\subsection{Deployment Checklist}

For practitioners implementing Robust Deep MCCFR, we recommend the following deployment protocol:

\subsubsection{Phase 1: Initial Setup}
\begin{enumerate}
\item Implement basic MCCFR with neural function approximation
\item Add diagnostic monitoring for risk indicators
\item Establish baseline performance metrics
\end{enumerate}

\subsubsection{Phase 2: Core Risk Mitigation}
\begin{enumerate}
\item \textbf{Prioritize target networks} with $\tau = 200$ (most consistent improvement across scales)
\item \textbf{Consider importance weight clipping carefully} - may help in small games but hurt in larger ones
\item \textbf{Use minimal exploration mixing} with $\epsilon = 0.05$ or consider disabling entirely
\item Add baseline subtraction (critical for performance)
\item Monitor weight statistics and exploitability trends
\end{enumerate}

\subsubsection{Phase 3: Advanced Features}
\begin{enumerate}
\item Add experience replay buffer (if computational resources allow)
\item Consider variance-aware training objectives (minimal impact observed)
\item Fine-tune component combinations based on game scale
\end{enumerate}

\subsubsection{Phase 4: Optimization}
\begin{enumerate}
\item Tune hyperparameters based on diagnostic feedback
\item Optimize computational efficiency
\item Validate on larger game instances
\end{enumerate}

\subsection{Diagnostic Interpretation Guide}

\begin{table}[H]
\centering
\caption{Diagnostic Indicator Interpretation}
\begin{tabular}{@{}llll@{}}
\toprule
Metric & Good Range & Warning Range & Action Required \\
\midrule
Support Entropy & $> 0.5$ & $0.2 - 0.5$ & $< 0.2$ \\
Max Importance Weight & $< 2C$ & $2C - 5C$ & $> 5C$ \\
Strategy Disagreement & $< 0.1$ & $0.1 - 0.5$ & $> 0.5$ \\
Training Loss Variance & $< 0.01$ & $0.01 - 0.1$ & $> 0.1$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Response Actions}:
\begin{itemize}
\item \textbf{High Exploitability}: First ensure target networks are enabled, then try removing exploration mixing or weight clipping
\item \textbf{High Importance Weights}: Consider removing weight clipping in larger games, as it may be counterproductive
\item \textbf{Training Instability}: Add target networks with $\tau = 200$, ensure baseline subtraction is enabled
\item \textbf{Slow Convergence}: Reduce exploration parameter $\epsilon$ to 0.05 or disable entirely
\end{itemize}

\subsection{Computational Resource Planning}

\begin{table}[H]
\centering
\caption{Resource Requirements by Game Size}
\begin{tabular}{@{}lcccc@{}}
\toprule
Game Size & Information Sets & Memory (GB) & Training Time & Recommended Config \\
\midrule
Small ($< 10^3$) & $< 1K$ & 2-4 & Hours & Full Framework \\
Medium ($10^3 - 10^6$) & $1K - 1M$ & 8-32 & Days & Selective Components \\
Large ($> 10^6$) & $> 1M$ & $> 32$ & Weeks & Core Components Only \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusions}

This paper presents the first comprehensive analysis of theoretical risks in neural MCCFR and proposes a principled framework for addressing these challenges. Our key contributions include:

\subsection{Theoretical Contributions}

\begin{enumerate}
\item \textbf{Risk Characterization}: We formally identify and analyze four critical risks in neural MCCFR: non-stationary targets, support collapse, variance explosion, and warm-start bias.

\item \textbf{Convergence Analysis}: We establish theoretical conditions under which standard neural MCCFR approaches may fail and derive variance bounds for importance-weighted estimators.

\item \textbf{Mitigation Theory}: We prove that our proposed mitigation strategies address each identified risk while maintaining theoretical guarantees.
\end{enumerate}

\subsection{Methodological Contributions}

\begin{enumerate}
\item \textbf{Robust Framework}: We develop a comprehensive framework that systematically addresses all identified risks through coordinated mitigation strategies.

\item \textbf{Diagnostic System}: We provide real-time monitoring tools that enable early detection and response to problematic behaviors.

\item \textbf{Implementation Guidelines}: We offer practical deployment protocols that facilitate adoption in real-world applications.
\end{enumerate}

\subsection{Empirical Contributions}

\begin{enumerate}
\item \textbf{Multi-Scale Validation}: Through experiments on both Kuhn Poker (12 information sets) and Leduc Poker (936 information sets), we demonstrate that neural MCCFR risks scale with game complexity as theoretically predicted.

\item \textbf{Theory-Practice Reconciliation}: We resolve the apparent contradiction between theory and small-game results by showing that risks manifest clearly in larger games, with Leduc experiments validating all major theoretical claims.

\item \textbf{Component Effectiveness Analysis}: We achieve 35\% reduction in exploitability on Leduc Poker through selective component usage (0.2404 best vs 0.3703 minimal framework) and demonstrate that over-engineering can hurt performance.

\item \textbf{Scale-Dependent Guidelines}: We provide the first systematic analysis of when different mitigation components are beneficial, showing clear scale dependence in component effectiveness.
\end{enumerate}

\subsection{Impact and Future Directions}

Our work addresses a critical gap in the literature by providing both theoretical understanding and practical solutions for neural MCCFR deployment at scale. The multi-domain validation demonstrates that our framework successfully addresses the fundamental challenges of integrating neural networks with MCCFR.

\textbf{Practical Impact}: The framework enables neural MCCFR deployment on games with 100+ times more information sets than previously validated, with systematic risk mitigation that scales appropriately with game complexity. Our diagnostic tools provide real-time risk detection, making the approach practical for larger domains.

\textbf{Methodological Contributions}: We provide the first systematic analysis of scale-dependent risk mitigation in neural MCCFR, showing that component selection must adapt to game complexity. This methodology extends beyond MCCFR to other neural game-theoretic algorithms.

\textbf{Validation at Scale}: The Leduc Poker experiments (936 information sets) represent a significant scale-up from previous neural MCCFR validation, demonstrating effectiveness where theoretical risks actually manifest. This addresses the key limitation identified in preliminary work.

Future work should extend validation to even larger games (Texas Hold'em, multi-agent domains), develop adaptive component selection algorithms, and establish formal convergence guarantees for the complete framework. The principles established here provide a foundation for neural approaches to extensive-form games at unprecedented scales.

\subsection{Broader Implications}

Beyond extensive-form games, our scale-dependent risk analysis has broader implications for neural algorithm design. The finding that mitigation effectiveness depends critically on problem scale suggests that neural algorithm components should be selected based on complexity metrics rather than applied uniformly.

\textbf{Broader Impact}: The principles of scale-aware component selection, systematic risk mitigation, and multi-domain validation provide a methodology for integrating neural networks with classical algorithms across domains. This work demonstrates that theoretical predictions must be validated at appropriate scales to avoid misleading conclusions.

\textbf{Methodological Legacy}: Our approach of combining rigorous theoretical analysis with systematic multi-scale experimental validation provides a template for future research at the intersection of game theory and deep learning. The diagnostic framework and component interaction analysis methodology are broadly applicable to neural algorithm development.

\section*{Acknowledgments}

The authors thank the anonymous reviewers for their constructive feedback and suggestions that significantly improved this paper. We also acknowledge the computational resources provided by École Polytechnique that made the extensive experimental evaluation possible.

\bibliographystyle{abbrvnat}
\bibliography{references}

\appendix

\section{Detailed Experimental Results}

\subsection{Complete Ablation Study Data}

\begin{table}[H]
\centering
\caption{Complete Ablation Study Results}
\begin{tabular}{@{}lcccc@{}}
\toprule
Configuration & Final & Training & Performance & Rank \\
 & Exploitability & Time (s) & Rank & (out of 23) \\
\midrule
No Exploration Mixing & \textbf{0.0628} & 224.7 & Best & 1st \\
Target Update 200 & 0.0753 & 210.6 & 2nd Best & 2nd \\
Exploration 0.05 & 0.0776 & 210.6 & 3rd Best & 3rd \\
No Weight Clipping & 0.1130 & 223.4 & Mid-tier & 5th \\
No Prioritized Replay & 0.1483 & - & Lower-mid & 11th \\
Minimal Configuration & 0.1560 & 166.6 & Poor & 14th \\
No Variance Objective & 0.1629 & - & Poor & 17th \\
No Target Networks & 0.1642 & 189.7 & Poor & 19th \\
Baseline (Full) & 0.1720 & 220.0 & Worst & 21st \\
No Baseline Subtraction & 0.1830 & - & Worst & 23rd \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hyperparameter Sensitivity Details}

\begin{table}[H]
\centering
\caption{Exploration Parameter Sensitivity ($\epsilon$)}
\begin{tabular}{@{}lccc@{}}
\toprule
$\epsilon$ & Final Exploitability & Training Time (s) & Performance Rank \\
\midrule
0.05 & \textbf{0.0776} & 210.6 & 3rd \\
0.15 & 0.1479 & 209.4 & 10th \\
0.20 & 0.1631 & - & 18th \\
\bottomrule
\end{tabular}
\end{table}

\section{Implementation Details}

\subsection{Network Architecture Specifications}

\begin{lstlisting}[language=Python, caption=Neural Network Architecture]
class UltraDeepNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_actions, 
                 num_blocks, bottleneck_factor):
        super().__init__()
        self.input_layer = nn.Linear(input_size, hidden_size)
        
        # Residual blocks
        self.blocks = nn.ModuleList([
            ResidualBlock(hidden_size, bottleneck_factor) 
            for _ in range(num_blocks)
        ])
        
        self.output_layer = nn.Linear(hidden_size, num_actions)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        x = F.relu(self.input_layer(x))
        for block in self.blocks:
            x = block(x)
        x = self.output_layer(x)
        return self.softmax(x)
\end{lstlisting}

\subsection{Risk Mitigation Implementation}

\begin{lstlisting}[language=Python, caption=Exploration Mixing]
def get_exploration_strategy(self, neural_probs, legal_actions, epsilon):
    # Extract legal action probabilities
    legal_probs = neural_probs[legal_actions]
    legal_probs = legal_probs / legal_probs.sum()
    
    # Mix with uniform distribution
    uniform_probs = torch.ones_like(legal_probs) / len(legal_actions)
    mixed_probs = (1 - epsilon) * legal_probs + epsilon * uniform_probs
    
    # Create full action vector
    strategy = torch.zeros_like(neural_probs)
    strategy[legal_actions] = mixed_probs
    return strategy
\end{lstlisting}

\section{Convergence Proofs}

\subsection{Proof of Target Network Stability}

\begin{proof}
For target networks updated every $\tau$ steps, the target distribution remains constant between updates:
$$\mathcal{D}_t^{target} = \mathcal{D}_{\lfloor t/\tau \rfloor \cdot \tau}^{target}$$

Therefore, for $t \not\equiv 0 \pmod{\tau}$:
$$D_{KL}(\mathcal{D}_t^{target} \| \mathcal{D}_{t-1}^{target}) = 0$$

At update steps $t = k\tau$ for integer $k \geq 1$, the target network copies the main network, so:
$$\mathcal{D}_{k\tau}^{target} = \mathcal{D}_{k\tau}^{main}$$

The KL divergence at update steps can be bounded using the assumption on network output changes. Since the target distribution depends on the network outputs through the regret-matching strategy, and using standard inequalities relating KL divergence to supremum norm differences for discrete probability distributions:
$$D_{KL}(\mathcal{D}_{k\tau}^{target} \| \mathcal{D}_{(k-1)\tau}^{target}) \leq C \sum_{j=(k-1)\tau}^{k\tau-1} \|f_\theta^{j+1} - f_\theta^{j}\|_\infty \leq C \tau \delta_f$$

However, this bound is too loose. More precisely, at update step $k\tau$, the target network jumps from parameters $\theta^{(k-1)\tau}$ to $\theta^{k\tau}$, so:
$$D_{KL}(\mathcal{D}_{k\tau}^{target} \| \mathcal{D}_{(k-1)\tau}^{target}) \leq C \|f_{\theta^{k\tau}} - f_{\theta^{(k-1)\tau}}\|_\infty \leq C \delta_f$$

for some constant $C > 0$ depending on the game structure.

Taking the average over a period of $\tau$ steps:
$$\frac{1}{\tau} \sum_{j=1}^{\tau} \mathbb{E}[D_{KL}(\mathcal{D}_{t+j}^{target} \| \mathcal{D}_{t+j-1}^{target})] = \frac{1}{\tau} \cdot C \delta_f = \frac{C \delta_f}{\tau}$$

This shows that target networks provide stability by reducing the average KL divergence shift by a factor of $1/\tau$ compared to using main networks directly. The benefit comes from the fact that most steps have zero shift, with periodic bounded shifts at update times.
\end{proof}

\end{document}
